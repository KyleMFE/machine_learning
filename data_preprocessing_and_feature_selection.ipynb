{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing data to avoid \"garbage in, garbage out\"  \n",
    "1. data cleaning  \n",
    "Fill in or missing values. detect and remove noisy data and outliers\n",
    "2.  data normalization  \n",
    "Normalize data to reduce dimensions and noise\n",
    "3. data reduction  \n",
    "Sample data records or attributes for easier data handling\n",
    "4. text cleaning  \n",
    "Remove embedded characters which may cause data misalignment\n",
    "5. data discretization  \n",
    "Convert continuous attributes to categorical attributes for ease of use with certain machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with missing values?\n",
    "*  **deletion**  \n",
    "Remove records with missing values\n",
    "* **Dummy substitution**  \n",
    "Replace missing values with a dummy value:e.g. unknown for categorical or 0 for numerical values\n",
    "* **Mean substition**  \n",
    "If numerical, use mean value\n",
    "* **Frequent substitution**  \n",
    "If categorical, use the most frequent\n",
    "* **Regression substitution**  \n",
    "Use a regression model to replace missing values with regressed values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to deal with outliers?  \n",
    "\n",
    "* ** keep outliers**  \n",
    "In many applications, outliers provide crucial information.\n",
    "* ** exclude outliers**  \n",
    "  * Trimming/Truncation: Trimming discards the outliers\n",
    "  * Winsorising: Replace the outliers with the nearest \"non-suspect\" data  \n",
    "\n",
    "\n",
    "  \n",
    "Trimming or Winsorising less than 5% of data points **will not** likely affect the hypothesis testing outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization  \n",
    "re-scale numerical values to a specified range.\n",
    "* **min-max Normalization**  \n",
    "$$X_{norm} = \\frac{X-X_{min}}{X_{max}-X_{min}}$$   \n",
    "* **Z-score Normalization( Standardization)**  \n",
    "$$ z = \\frac{X-\\mu}{\\sigma}$$\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation\n",
    "* **Decimal scaling**  \n",
    "Scale the data by moving the decimal point of the attribute value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to discretize data?\n",
    "Many different values for some algorithms lead to very complex models. We can convert continous attributes by **\"binning\"** to categorical\n",
    "attributes for ease of use with certain machine learning methods  \n",
    "**Binning**  \n",
    "* Equal-Width Binning  \n",
    "0-10, 10-20, 20-30\n",
    "* Equal-Heigth Binnig(by frequency)  \n",
    "Interval with same number \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to reduce data?\n",
    "* **Record Sampling**  \n",
    "Sample the data records and only choose the representative subset from the data  \n",
    "* **Attribute Sampling**  \n",
    "Select only a subset of the most important attributes from the data  \n",
    "* ** Aggregation**  \n",
    "Divide the data into groups and store the numbers for each group. For example, the daily revenue\n",
    "numbers of a restaurant chain over the past 20 years can be aggregated to monthly revenue to reduce the size of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature Engineering\n",
    "* a sort of art  \n",
    "* augment data (**creative addtional relevant features** from the existing raw features to increase the predictive power of the learning algorithm  \n",
    "\n",
    "### Feature Selection\n",
    "* eleminate irrelevant, redundant, or highly correlated features\n",
    "* a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully\n",
    "* useful when dealing with high-dimensional data or dataset contains a large number of features and a limited number of observations\n",
    "\n",
    "#### Why?\n",
    "* simplification of models to make them easier to interpret\n",
    "* shorter training times and speed up learning process\n",
    "* to avoid the curse of dimensionality\n",
    "* enhanced generalization by reducing overfitting  \n",
    "  \n",
    "**Note:** As you add more features, you may **exponentially** need more data to kind of fill out the space.\n",
    "  \n",
    "#### Approaches for Feature Selection\n",
    "* Traditional approaches\n",
    " * Forward selection  \n",
    " Start with no variables. Iteratively add variables and test the profermance of the model until adding more variables no longer makes a positive effect\n",
    " * Backward selection  \n",
    " Proceed with removing variables and testing the predictive accurancy of the model. Begin with all the variables in the model\n",
    " * Stepwise selection  \n",
    " After each stage in the process, after a new variable is added, a test is made to check if some variables can be deleted without appreciabl increasing the error. The procedure terminates when the measure is maximized\n",
    "* Modern approaches\n",
    " * Filter method  \n",
    " By evaluating the correlation between each feature and  the target attribute. Apply a statistical measure to assign a score to each variable.\n",
    " Then rank the features by the score and then choose features.  \n",
    " Measures: Pearson correlation, Mutual information, Chi squared test  \n",
    " **Problems:** Tend to select redundant variables for not considering the relationships between variables. Used as pre-process method\n",
    "  * Wrapper method  (** Cross validation**)\n",
    "  * Embedded method  (**Regularization method:** most used)  \n",
    "  Learn with features best contribute to the accurancy of the model while the model is being created  \n",
    "  Implemented by algorithms that have their own bulit in feature selection methods such as LASSO,RIDGE \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection VS Dimension reduction\n",
    "* Feature selection methods extract a subset of original features in the data without **changing them**.  \n",
    "* Dimension reduction method can **transform the original features and thus modify them**.  \n",
    "Methods: PCA, canonical correlation analysis and Singular Value Decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
